{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d9de5c-5b9b-4c5e-9b87-fd3f99d64a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: 2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy<2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a3933b-6e70-479e-8c37-6b61a1c0fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78cb5d24-96a8-4b69-8737-3ba45c363cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import gradio as gr\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hugging Face imports\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Metrics imports\n",
    "try:\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    from nltk.translate.meteor_score import meteor_score\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    METRICS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"NLTK not installed. BLEU and METEOR metrics will not be available.\")\n",
    "    METRICS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    IMAGE_METRICS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"scikit-image not installed. PSNR and SSIM metrics will not be available.\")\n",
    "    IMAGE_METRICS_AVAILABLE = False\n",
    "\n",
    "# Configure \n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set seed \n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) if torch.cuda.is_available() else None\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything()\n",
    "\n",
    "# Global tokenizer\n",
    "tokenizer = None\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "780dc8cb-0357-4074-ac8a-1b23df56b016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset path: /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"/Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K\"\n",
    "print(f\"Using dataset path: {DATASET_PATH}\")\n",
    "\n",
    "IMAGE_DIR = os.path.join(DATASET_PATH, \"Images\")\n",
    "if not os.path.exists(IMAGE_DIR):\n",
    "    IMAGE_DIR = os.path.join(DATASET_PATH, \"Flickr8k_Dataset\")\n",
    "    if not os.path.exists(IMAGE_DIR):\n",
    "        print(f\"Warning: Could not find image directory in {DATASET_PATH}\")\n",
    "\n",
    "CAPTION_FILE = os.path.join(DATASET_PATH, \"captions.txt\")\n",
    "if not os.path.exists(CAPTION_FILE):\n",
    "    alt_caption_files = [\n",
    "        os.path.join(DATASET_PATH, \"Flickr8k.token.txt\"),\n",
    "        os.path.join(DATASET_PATH, \"Flickr_8k.lemma.token.txt\"),\n",
    "        os.path.join(DATASET_PATH, \"Flickr8k_text\", \"Flickr8k.token.txt\")\n",
    "    ]\n",
    "    \n",
    "    for alt_file in alt_caption_files:\n",
    "        if os.path.exists(alt_file):\n",
    "            CAPTION_FILE = alt_file\n",
    "            print(f\"Found caption file at: {CAPTION_FILE}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Warning: Could not find caption file in {DATASET_PATH}\")\n",
    "\n",
    "WORK_DIR = os.path.join(DATASET_PATH, \"output\")\n",
    "FEATURE_DIR = os.path.join(WORK_DIR, \"features\")\n",
    "CHECKPOINT_DIR = os.path.join(WORK_DIR, \"checkpoints\")\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Define image transforms for ResNet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define image transforms for diffusion model\n",
    "diffusion_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d46293a-5910-4f99-992c-5be6f433d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing GPT-2 tokenizer...\n",
      "Tokenizer vocabulary size: 50257\n",
      "Loaded captions for 8091 images\n",
      "Sample of captions:\n",
      "Image: 1000268201_693b08cb0e.jpg, Caption: A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "Image: 1001773457_577c3a7d70.jpg, Caption: A black dog and a spotted dog are fighting\n",
      "Image: 1002674143_1b742ab4b8.jpg, Caption: A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer function\n",
    "def initialize_tokenizer():\n",
    "    global tokenizer\n",
    "    print(\"Initializing GPT-2 tokenizer...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "    return tokenizer\n",
    "\n",
    "# Load captions\n",
    "def load_captions(caption_file):\n",
    "    \"\"\"Load captions from Flickr8k captions file\"\"\"\n",
    "    captions_dict = defaultdict(list)\n",
    "    \n",
    "    if not os.path.exists(caption_file):\n",
    "        print(f\"Caption file not found: {caption_file}\")\n",
    "        return captions_dict\n",
    "        \n",
    "    try:\n",
    "        with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline().strip()\n",
    "            f.seek(0)  # Go back to start of file\n",
    "            \n",
    "            if '#' in first_line and '\\t' in first_line:  \n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) == 2:\n",
    "                        img_name = parts[0].split('#')[0]  \n",
    "                        caption = parts[1].strip()\n",
    "                        captions_dict[img_name].append(caption)\n",
    "            else:  \n",
    "                next(f, None)  \n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    parts = line.split(',', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        img_name = parts[0].strip()\n",
    "                        caption = parts[1].strip()\n",
    "                        captions_dict[img_name].append(caption)\n",
    "                        \n",
    "        print(f\"Loaded captions for {len(captions_dict)} images\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading captions: {e}\")\n",
    "    \n",
    "    return captions_dict\n",
    "\n",
    "initialize_tokenizer()\n",
    "\n",
    "# Sample captions\n",
    "test_captions = load_captions(CAPTION_FILE)\n",
    "sample_keys = list(test_captions.keys())[:3]\n",
    "print(f\"Sample of captions:\")\n",
    "for key in sample_keys:\n",
    "    print(f\"Image: {key}, Caption: {test_captions[key][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10cb5584-8d5a-4dc5-a5ab-0f1d8ee95d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset classes created.\n"
     ]
    }
   ],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, image_dir, captions_dict, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.transform = transform\n",
    "        self.image_files = list(captions_dict.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Check if image exists\n",
    "        if not os.path.exists(img_path):\n",
    "            for ext in ['.jpg', '.jpeg', '.png']:\n",
    "                alt_path = os.path.join(self.image_dir, os.path.splitext(img_name)[0] + ext)\n",
    "                if os.path.exists(alt_path):\n",
    "                    img_path = alt_path\n",
    "                    break\n",
    "            else:\n",
    "                return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        caption = random.choice(self.captions_dict[img_name])\n",
    "        \n",
    "        return image, caption, img_name\n",
    "\n",
    "class FeatureCaptionDataset(Dataset):\n",
    "    def __init__(self, feature_dir, captions_dict, tokenizer, max_len=50):\n",
    "        self.feature_dir = feature_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_files = list(captions_dict.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        feature_path = os.path.join(self.feature_dir, img_name.replace('.jpg', '.npy'))\n",
    "        \n",
    "        if not os.path.exists(feature_path):\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            for ext in ['.npy']:\n",
    "                alt_path = os.path.join(self.feature_dir, base_name + ext)\n",
    "                if os.path.exists(alt_path):\n",
    "                    feature_path = alt_path\n",
    "                    break\n",
    "            else:\n",
    "                return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        # Load features\n",
    "        feature = torch.tensor(np.load(feature_path), dtype=torch.float32)\n",
    "        \n",
    "        # Get random caption\n",
    "        caption = random.choice(self.captions_dict[img_name])\n",
    "        \n",
    "        # Tokenize caption\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return feature, tokens.input_ids.squeeze(0), tokens.attention_mask.squeeze(0), caption\n",
    "\n",
    "print(\"Dataset classes created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4878f4ec-46ef-41f9-9a7d-82f25cf7ebcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features already exist for sample images\n"
     ]
    }
   ],
   "source": [
    "# Extract features using ResNet50\n",
    "def extract_features(dataset, feature_dir, batch_size=32):\n",
    "    \"\"\"Extract features from images using ResNet50\"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Create ResNet model\n",
    "    resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    feature_extractor = nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    print(f\"Extracting features for {len(dataset)} images...\")\n",
    "    for images, _, img_names in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(images).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        # Save features\n",
    "        for i, img_name in enumerate(img_names):\n",
    "            feature_path = os.path.join(feature_dir, os.path.splitext(img_name)[0] + '.npy')\n",
    "            np.save(feature_path, features[i].cpu().numpy())\n",
    "    \n",
    "    print(\"Feature extraction complete\")\n",
    "if len(test_captions) > 0:\n",
    "    mini_dataset = FlickrDataset(IMAGE_DIR, {k: test_captions[k] for k in sample_keys}, transform=transform)\n",
    "\n",
    "    need_extraction = False\n",
    "    for key in sample_keys:\n",
    "        feature_path = os.path.join(FEATURE_DIR, os.path.splitext(key)[0] + '.npy')\n",
    "        if not os.path.exists(feature_path):\n",
    "            need_extraction = True\n",
    "            break\n",
    "    \n",
    "    if need_extraction and len(mini_dataset) > 0:\n",
    "        extract_features(mini_dataset, FEATURE_DIR, batch_size=2)\n",
    "        print(\"Sample features extracted\")\n",
    "    else:\n",
    "        print(\"Features already exist for sample images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e97e0285-93c2-4443-ad10-14d6aa37b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created caption model with 126013440 parameters\n",
      "Sample image: 1000268201_693b08cb0e.jpg\n",
      "Actual caption: A child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "Generated caption: In this section, we provide a simple way to generate a string that will fit in your current state.\n"
     ]
    }
   ],
   "source": [
    "class GPT2CaptionModel(nn.Module):\n",
    "    def __init__(self, feature_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.project = nn.Linear(feature_dim, embed_dim)\n",
    "        self.gpt2.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    def forward(self, features, input_ids, attention_mask):\n",
    "        prefix = self.project(features).unsqueeze(1)\n",
    "        token_embed = self.gpt2.transformer.wte(input_ids)\n",
    "        embed = torch.cat([prefix, token_embed], dim=1)\n",
    "        extended_mask = torch.cat([\n",
    "            torch.ones((input_ids.shape[0], 1), dtype=attention_mask.dtype, device=features.device),\n",
    "            attention_mask\n",
    "        ], dim=1)\n",
    "        labels = F.pad(input_ids, (1, 0), value=-100)\n",
    "        outputs = self.gpt2(\n",
    "            inputs_embeds=embed,\n",
    "            attention_mask=extended_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        return outputs.loss, outputs.logits\n",
    "    \n",
    "    def generate_caption(self, feature_tensor, max_len=30, temperature=0.9, top_k=40):\n",
    "        \"\"\"Generate a caption for an image feature tensor with fixed decoding\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        feature_tensor = feature_tensor.to(next(self.parameters()).device)\n",
    "        if feature_tensor.dim() == 1:\n",
    "            feature_tensor = feature_tensor.unsqueeze(0)\n",
    "            \n",
    "        if feature_tensor.size(0) > 1:\n",
    "            captions = []\n",
    "            for i in range(feature_tensor.size(0)):\n",
    "                single_caption = self.generate_caption(\n",
    "                    feature_tensor[i].unsqueeze(0), \n",
    "                    max_len=max_len, \n",
    "                    temperature=temperature\n",
    "                )\n",
    "                captions.append(single_caption)\n",
    "            return captions[0] if len(captions) == 1 else captions\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prefix = self.project(feature_tensor).unsqueeze(1)\n",
    "            \n",
    "            # Start with a special token\n",
    "            bos_token_id = tokenizer.bos_token_id if hasattr(tokenizer, 'bos_token_id') else tokenizer.eos_token_id\n",
    "            if bos_token_id is None:\n",
    "                bos_token_id = 50256  \n",
    "                \n",
    "            input_ids = torch.tensor([[bos_token_id]]).to(feature_tensor.device)\n",
    "            \n",
    "            generated_text = \"\"\n",
    "            \n",
    "            for _ in range(max_len):\n",
    "                token_embed = self.gpt2.transformer.wte(input_ids)\n",
    "                if input_ids.size(1) == 1:\n",
    "                    inputs_embeds = torch.cat([prefix, token_embed], dim=1)\n",
    "                    attention_mask = torch.ones((1, 2), device=feature_tensor.device)\n",
    "                else:\n",
    "                    inputs_embeds = token_embed\n",
    "                    attention_mask = torch.ones((1, input_ids.size(1)), device=feature_tensor.device)\n",
    "                outputs = self.gpt2(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "                next_token_logits = outputs.logits[:, -1, :] / temperature\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "                next_token_logits.fill_(-float('inf'))\n",
    "                next_token_logits.scatter_(1, top_k_indices, top_k_logits)\n",
    "\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "                \n",
    "                token_str = tokenizer.decode([next_token.item()], skip_special_tokens=True)\n",
    "                generated_text += token_str\n",
    "                \n",
    "                if next_token.item() == tokenizer.eos_token_id or token_str.strip() == \".\":\n",
    "                    break\n",
    "  \n",
    "            if not generated_text.strip():\n",
    "                return \"A photo.\"\n",
    "                \n",
    "            return generated_text\n",
    "\n",
    "mini_caption_model = GPT2CaptionModel().to(device)\n",
    "print(f\"Created caption model with {sum(p.numel() for p in mini_caption_model.parameters())} parameters\")\n",
    "\n",
    "# Load a sample feature to test caption generation\n",
    "if os.path.exists(os.path.join(FEATURE_DIR, os.path.splitext(sample_keys[0])[0] + '.npy')):\n",
    "    sample_feature = torch.tensor(\n",
    "        np.load(os.path.join(FEATURE_DIR, os.path.splitext(sample_keys[0])[0] + '.npy')), \n",
    "        dtype=torch.float32\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate a sample caption\n",
    "    sample_caption = mini_caption_model.generate_caption(sample_feature, temperature=0.8)\n",
    "    print(f\"Sample image: {sample_keys[0]}\")\n",
    "    print(f\"Actual caption: {test_captions[sample_keys[0]][0]}\")\n",
    "    print(f\"Generated caption: {sample_caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8bb9d97-df93-41cb-9fbb-9b736973ace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved training and evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Training function for caption model\n",
    "def train_caption_model(model, train_loader, val_loader=None, epochs=5, lr=1e-4):\n",
    "    \"\"\"Train the caption model\"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (feature, input_ids, attention_mask, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "            feature = feature.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            loss, _ = model(feature, input_ids, attention_mask)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # Generate a sample caption to check progress\n",
    "                if batch_idx % 200 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        sample_caption = model.generate_caption(feature[0].unsqueeze(0))\n",
    "                        print(f\"Sample caption: {sample_caption}\")\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for feature, input_ids, attention_mask, _ in tqdm(val_loader, desc=\"Validation\"):\n",
    "                    feature = feature.to(device)\n",
    "                    input_ids = input_ids.to(device)\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                    \n",
    "                    loss, _ = model(feature, input_ids, attention_mask)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "            \n",
    "            print(\"Sample validation captions:\")\n",
    "            for i in range(min(3, len(feature))):\n",
    "                with torch.no_grad():\n",
    "                    caption = model.generate_caption(feature[i].unsqueeze(0))\n",
    "                    print(f\"Generated: {caption}\")\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_path = os.path.join(CHECKPOINT_DIR, \"caption_model_best.pt\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"New best model saved to {best_model_path}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"caption_model_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_path = os.path.join(CHECKPOINT_DIR, \"caption_model_final.pt\")\n",
    "    torch.save(model.state_dict(), final_path)\n",
    "    print(f\"Final model saved to {final_path}\")\n",
    "    \n",
    "    if val_loader is not None and os.path.exists(os.path.join(CHECKPOINT_DIR, \"caption_model_best.pt\")):\n",
    "        model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, \"caption_model_best.pt\")))\n",
    "        print(\"Loaded best model based on validation loss\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_caption_model(model, dataloader, num_samples=10):\n",
    "    \"\"\"Evaluate the caption model and print example outputs\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    samples = []\n",
    "    metrics = {'bleu1': [], 'bleu4': []}\n",
    "    \n",
    "    if not METRICS_AVAILABLE:\n",
    "        print(\"NLTK metrics not available for evaluation.\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            feature, _, _, true_caption = batch\n",
    "            \n",
    "            for i in range(feature.size(0)):\n",
    "                single_feature = feature[i].unsqueeze(0).to(device)\n",
    "                single_caption = true_caption[i]\n",
    "                \n",
    "                generated_caption = model.generate_caption(\n",
    "                    single_feature, \n",
    "                    temperature=0.8,\n",
    "                    top_k=40\n",
    "                )\n",
    "                \n",
    "                # Print debugging info\n",
    "                print(f\"Feature shape: {single_feature.shape}\")\n",
    "                print(f\"Generated caption: '{generated_caption}'\")\n",
    "                \n",
    "                # Store sample\n",
    "                samples.append({\n",
    "                    'true': single_caption,\n",
    "                    'generated': generated_caption\n",
    "                })\n",
    "                \n",
    "                if METRICS_AVAILABLE and generated_caption.strip():\n",
    "                    # BLEU score\n",
    "                    reference = [single_caption.split()]\n",
    "                    candidate = generated_caption.split()\n",
    "                    \n",
    "                    # Skip empty candidates\n",
    "                    if not candidate:\n",
    "                        continue\n",
    "                    \n",
    "                    # BLEU-1\n",
    "                    bleu1 = sentence_bleu(reference, candidate, \n",
    "                                         weights=(1, 0, 0, 0),\n",
    "                                         smoothing_function=SmoothingFunction().method1)\n",
    "                    metrics['bleu1'].append(bleu1)\n",
    "                    \n",
    "                    # BLEU-4\n",
    "                    bleu4 = sentence_bleu(reference, candidate,\n",
    "                                         weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                                         smoothing_function=SmoothingFunction().method1)\n",
    "                    metrics['bleu4'].append(bleu4)\n",
    "                \n",
    "                if len(samples) >= num_samples:\n",
    "                    break\n",
    "            \n",
    "            if len(samples) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nCaption Generation Examples:\")\n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"True: {sample['true']}\")\n",
    "        print(f\"Generated: {sample['generated']}\")\n",
    "        print()\n",
    "    \n",
    "    # Print metrics if available\n",
    "    if METRICS_AVAILABLE and len(metrics['bleu1']) > 0:\n",
    "        print(\"Metrics:\")\n",
    "        print(f\"BLEU-1: {sum(metrics['bleu1']) / len(metrics['bleu1']):.4f}\")\n",
    "        print(f\"BLEU-4: {sum(metrics['bleu4']) / len(metrics['bleu4']):.4f}\")\n",
    "    else:\n",
    "        print(\"No metrics available or all generated captions were empty\")\n",
    "    \n",
    "    return samples, metrics\n",
    "\n",
    "print(\"Improved training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b384b25b-db45-43fa-b0b0-825a734c3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def visualize_caption_results(samples):\n",
    "    \"\"\"Create a visualization grid of caption results\"\"\"\n",
    "    plt.figure(figsize=(12, len(samples) * 2))\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        plt.subplot(len(samples), 1, i + 1)\n",
    "        plt.text(0.5, 0.5, f\"True: {sample['true']}\\nGenerated: {sample['generated']}\", \n",
    "                 ha='center', va='center', fontsize=12)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "def visualize_denoising_results(samples):\n",
    "    \"\"\"Create a visualization grid of denoising results\"\"\"\n",
    "    plt.figure(figsize=(12, len(samples) * 4))\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        original = sample['original'].permute(1, 2, 0).numpy()\n",
    "        original = (original + 1) / 2  # Convert from [-1,1] to [0,1]\n",
    "        \n",
    "        noisy = sample['noisy'].permute(1, 2, 0).numpy()\n",
    "        noisy = (noisy + 1) / 2  # Convert from [-1,1] to [0,1]\n",
    "        \n",
    "        denoised = sample['denoised'].permute(1, 2, 0).numpy()\n",
    "        denoised = (denoised + 1) / 2  # Convert from [-1,1] to [0,1]\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(len(samples), 3, i * 3 + 1)\n",
    "        plt.imshow(np.clip(original, 0, 1))\n",
    "        plt.title('Original')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Noisy image\n",
    "        plt.subplot(len(samples), 3, i * 3 + 2)\n",
    "        plt.imshow(np.clip(noisy, 0, 1))\n",
    "        plt.title('Noisy')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Denoised image\n",
    "        plt.subplot(len(samples), 3, i * 3 + 3)\n",
    "        plt.imshow(np.clip(denoised, 0, 1))\n",
    "        plt.title('Denoised')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "def tensor_to_pil(tensor):\n",
    "    \"\"\"Convert a tensor to PIL image for display\"\"\"\n",
    "    tensor = tensor.cpu().clone()\n",
    "    tensor = tensor.squeeze(0)\n",
    "    \n",
    "    if tensor.min() < 0:\n",
    "        tensor = (tensor + 1) / 2\n",
    "    \n",
    "    # Convert to PIL\n",
    "    tensor = tensor.clamp(0, 1)\n",
    "    tensor = tensor.permute(1, 2, 0).numpy() * 255\n",
    "    return Image.fromarray(tensor.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fa2d6f4-dc7f-460d-b6e8-bdd653ad4001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Starting image captioning and denoising pipeline...\")\n",
    "    \n",
    "    print(\"Loading captions...\")\n",
    "    captions_dict = load_captions(CAPTION_FILE)\n",
    "    if len(captions_dict) == 0:\n",
    "        print(\"No captions found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    print(\"Creating dataset for feature extraction...\")\n",
    "    dataset = FlickrDataset(IMAGE_DIR, captions_dict, transform=transform)\n",
    "    \n",
    "    if len(os.listdir(FEATURE_DIR)) < len(dataset) * 0.8:\n",
    "        print(\"Extracting features...\")\n",
    "        extract_features(dataset, FEATURE_DIR)\n",
    "    else:\n",
    "        print(\"Features already exist, skipping extraction.\")\n",
    "    \n",
    "    # Initialize GPT-2 tokenizer (globally)\n",
    "    print(\"Initializing tokenizer...\")\n",
    "    initialize_tokenizer()\n",
    "    \n",
    "    print(\"Creating feature-caption dataset...\")\n",
    "\n",
    "    print(\"Creating feature-caption dataset...\")\n",
    "    feature_dataset = FeatureCaptionDataset(FEATURE_DIR, captions_dict, tokenizer)\n",
    "    \n",
    "    # Split data\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(feature_dataset)), test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        [feature_dataset[i] for i in train_indices],\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=0 \n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        [feature_dataset[i] for i in val_indices],\n",
    "        batch_size=16,  \n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(\"Initializing caption model...\")\n",
    "    caption_model = GPT2CaptionModel().to(device)\n",
    "   \n",
    "    pretrained_path = os.path.join(CHECKPOINT_DIR, \"caption_model_final.pt\")\n",
    "    if os.path.exists(pretrained_path):\n",
    "        print(f\"Loading pretrained model from {pretrained_path}\")\n",
    "        caption_model.load_state_dict(torch.load(pretrained_path))\n",
    "    else:\n",
    "        print(\"Training caption model...\")\n",
    "        caption_model = train_caption_model(\n",
    "            caption_model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=3,\n",
    "            lr=5e-5\n",
    "        )\n",
    "    \n",
    "    print(\"Evaluating caption model...\")\n",
    "    caption_samples, caption_metrics = evaluate_caption_model(\n",
    "        caption_model,\n",
    "        val_loader,\n",
    "        num_samples=5\n",
    "    )\n",
    "    \n",
    "    print(\"Creating dataset for denoising model...\")\n",
    "    image_dataset = FlickrDataset(IMAGE_DIR, captions_dict, transform=diffusion_transform)\n",
    "    \n",
    "    # Split data\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(image_dataset)), test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    denoising_train_loader = DataLoader(\n",
    "        [image_dataset[i] for i in train_indices],\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=0  \n",
    "    )\n",
    "    \n",
    "    denoising_val_loader = DataLoader(\n",
    "        [image_dataset[i] for i in val_indices],\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Train denoiser model\n",
    "    print(\"Initializing denoiser model...\")\n",
    "    denoiser_model = SimpleDenoiser().to(device)\n",
    "    \n",
    "    # Check if pretrained model exists\n",
    "    pretrained_denoiser_path = os.path.join(CHECKPOINT_DIR, \"denoiser_model_final.pt\")\n",
    "    if os.path.exists(pretrained_denoiser_path):\n",
    "        print(f\"Loading pretrained denoiser model from {pretrained_denoiser_path}\")\n",
    "        denoiser_model.load_state_dict(torch.load(pretrained_denoiser_path))\n",
    "    else:\n",
    "        print(\"Training denoiser model...\")\n",
    "        denoiser_model = train_denoiser(\n",
    "            denoiser_model,\n",
    "            denoising_train_loader,\n",
    "            denoising_val_loader,\n",
    "            epochs=3,\n",
    "            lr=1e-4\n",
    "        )\n",
    "    \n",
    "    # Evaluate denoiser model\n",
    "    print(\"Evaluating denoiser model...\")\n",
    "    denoising_samples, denoising_metrics = evaluate_denoiser(\n",
    "        denoiser_model,\n",
    "        denoising_val_loader,\n",
    "        num_samples=5\n",
    "    )\n",
    "    \n",
    "    print(\"Pipeline complete!\")\n",
    "    \n",
    "    # Return models and evaluation results\n",
    "    return {\n",
    "        'caption_model': caption_model,\n",
    "        'denoiser_model': denoiser_model,\n",
    "        'caption_samples': caption_samples,\n",
    "        'caption_metrics': caption_metrics,\n",
    "        'denoising_samples': denoising_samples,\n",
    "        'denoising_metrics': denoising_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "086e3881-3a4e-47eb-8757-9c0383f3b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleDenoiser model\n",
    "class SimpleDenoiser(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        # Middle\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(hidden_channels, hidden_channels*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels*2, hidden_channels*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_channels*2, hidden_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, in_channels, kernel_size=3, padding=1),\n",
    "            nn.Tanh()  # Output in [-1, 1] range\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "def add_noise_to_image(image_tensor, noise_level=0.1):\n",
    "    \"\"\"Add random noise to image tensor\"\"\"\n",
    "    noise = torch.randn_like(image_tensor) * noise_level\n",
    "    noisy_image = image_tensor + noise\n",
    "    return torch.clamp(noisy_image, -1, 1)\n",
    "\n",
    "def train_denoiser(model, train_loader, val_loader=None, epochs=5, lr=1e-4):\n",
    "    \"\"\"Train the denoiser model\"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (images, _, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Add noise to images\n",
    "            noisy_images = add_noise_to_image(images, noise_level=0.1)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            denoised_images = model(noisy_images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(denoised_images, images)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "     \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, _, _ in tqdm(val_loader, desc=\"Validation\"):\n",
    "                    images = images.to(device)\n",
    "                    \n",
    "                    noisy_images = add_noise_to_image(images, noise_level=0.1)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    denoised_images = model(noisy_images)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = criterion(denoised_images, images)\n",
    "                    val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "            \n",
    "            avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_path = os.path.join(CHECKPOINT_DIR, \"denoiser_model_best.pt\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"New best model saved to {best_model_path}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"denoiser_model_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_path = os.path.join(CHECKPOINT_DIR, \"denoiser_model_final.pt\")\n",
    "    torch.save(model.state_dict(), final_path)\n",
    "    print(f\"Final model saved to {final_path}\")\n",
    "    \n",
    "    # Load best model if available\n",
    "    if val_loader is not None and os.path.exists(os.path.join(CHECKPOINT_DIR, \"denoiser_model_best.pt\")):\n",
    "        model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, \"denoiser_model_best.pt\")))\n",
    "        print(\"Loaded best model based on validation loss\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluation function for denoiser model\n",
    "def evaluate_denoiser(model, dataloader, num_samples=5):\n",
    "    \"\"\"Evaluate the denoiser model and display example outputs\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    samples = []\n",
    "    metrics = {'psnr': [], 'ssim': []}\n",
    "    \n",
    "    if not IMAGE_METRICS_AVAILABLE:\n",
    "        print(\"Image metrics not available for evaluation.\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Add noise to images\n",
    "            noisy_images = add_noise_to_image(images, noise_level=0.1)\n",
    "            \n",
    "            # Denoise\n",
    "            denoised_images = model(noisy_images)\n",
    "            \n",
    "            # Calculate metrics if available\n",
    "            if IMAGE_METRICS_AVAILABLE:\n",
    "                for i in range(images.size(0)):\n",
    "                    # Convert tensors to numpy arrays for metric calculation\n",
    "                    clean_np = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                    clean_np = (clean_np + 1) / 2  # Convert from [-1,1] to [0,1]\n",
    "                    \n",
    "                    denoised_np = denoised_images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                    denoised_np = (denoised_np + 1) / 2  # Convert from [-1,1] to [0,1]\n",
    "                    \n",
    "                    # Calculate PSNR\n",
    "                    psnr_val = psnr(clean_np, denoised_np, data_range=1.0)\n",
    "                    metrics['psnr'].append(psnr_val)\n",
    "                    \n",
    "                    # Calculate SSIM\n",
    "                    ssim_val = ssim(clean_np, denoised_np, data_range=1.0, multichannel=True)\n",
    "                    metrics['ssim'].append(ssim_val)\n",
    "            \n",
    "            # Store sample images\n",
    "            for i in range(min(images.size(0), num_samples - len(samples))):\n",
    "                samples.append({\n",
    "                    'original': images[i].cpu(),\n",
    "                    'noisy': noisy_images[i].cpu(),\n",
    "                    'denoised': denoised_images[i].cpu()\n",
    "                })\n",
    "            \n",
    "            if len(samples) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Print metrics \n",
    "    if IMAGE_METRICS_AVAILABLE and len(metrics['psnr']) > 0:\n",
    "        print(\"Image Denoising Metrics:\")\n",
    "        print(f\"PSNR: {sum(metrics['psnr']) / len(metrics['psnr']):.4f} dB\")\n",
    "        print(f\"SSIM: {sum(metrics['ssim']) / len(metrics['ssim']):.4f}\")\n",
    "    \n",
    "    return samples, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb8a9102-150a-4b87-89ae-c38702b60260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Gradio interface\n",
    "def setup_gradio_interface(caption_model, denoiser_model):\n",
    "    def process_image(input_image):\n",
    "        input_image_pil = Image.fromarray(input_image).convert('RGB')\n",
    "        \n",
    "        input_tensor = transform(input_image_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "            feature_extractor = nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
    "            feature_extractor.eval()\n",
    "            features = feature_extractor(input_tensor).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        # Generate caption\n",
    "        caption = caption_model.generate_caption(features, temperature=0.7)\n",
    "        \n",
    "        # Process for denoising\n",
    "        noisy_tensor = diffusion_transform(input_image_pil).unsqueeze(0).to(device)\n",
    "        noisy_tensor = add_noise_to_image(noisy_tensor, noise_level=0.2)\n",
    "        \n",
    "        # Denoise\n",
    "        with torch.no_grad():\n",
    "            denoised_tensor = denoiser_model(noisy_tensor)\n",
    "        \n",
    "        # Convert tensors to images\n",
    "        noisy_image = tensor_to_pil(noisy_tensor)\n",
    "        denoised_image = tensor_to_pil(denoised_tensor)\n",
    "        \n",
    "        return caption, noisy_image, denoised_image\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    interface = gr.Interface(\n",
    "        fn=process_image,\n",
    "        inputs=gr.Image(),\n",
    "        outputs=[\n",
    "            gr.Textbox(label=\"Generated Caption\"),\n",
    "            gr.Image(label=\"Noisy Image\"),\n",
    "            gr.Image(label=\"Denoised Image\")\n",
    "        ],\n",
    "        title=\"Image Captioning and Denoising Demo\",\n",
    "        description=\"Upload an image to generate a caption and see denoising in action.\"\n",
    "    )\n",
    "    \n",
    "    return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6895799-19a4-40cf-b2b2-6710034cfcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with a small dataset...\n",
      "Loaded captions for 8091 images\n",
      "Initializing GPT-2 tokenizer...\n",
      "Tokenizer vocabulary size: 50257\n",
      "Training caption model (this may take some time even with small dataset)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|                                         | 0/10 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1/5:  10%|███▎                             | 1/10 [00:01<00:10,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 7.7736\n",
      "Sample caption: (Editor's note: This story was originally published on Nov.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|████████████████████████████████| 10/10 [00:06<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 2.4130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9414\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "New best model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_best.pt\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.9679\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average Loss: 0.8113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8487\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "New best model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_best.pt\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.6994\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average Loss: 0.6358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8222\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "New best model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_best.pt\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.6532\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average Loss: 0.4920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8840\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.4393\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average Loss: 0.3509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9505\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_5.pt\n",
      "Final model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_final.pt\n",
      "Loaded best model based on validation loss\n",
      "Evaluating caption model...\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "\n",
      "Caption Generation Examples:\n",
      "Example 1:\n",
      "True: two dogs running around\n",
      "Generated: A photo.\n",
      "\n",
      "Example 2:\n",
      "True: A dog and a tennis ball .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 3:\n",
      "True: Two people wearing yellow jackets cross-country skiing .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 4:\n",
      "True: A boy descends off the end of a high diving board .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 5:\n",
      "True: A brown dog is running after the black dog .\n",
      "Generated: A photo.\n",
      "\n",
      "Metrics:\n",
      "BLEU-1: 0.0107\n",
      "BLEU-4: 0.0032\n",
      "\n",
      "Caption Results:\n",
      "Example 1:\n",
      "True: two dogs running around\n",
      "Generated: A photo.\n",
      "\n",
      "Example 2:\n",
      "True: A dog and a tennis ball .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 3:\n",
      "True: Two people wearing yellow jackets cross-country skiing .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 4:\n",
      "True: A boy descends off the end of a high diving board .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 5:\n",
      "True: A brown dog is running after the black dog .\n",
      "Generated: A photo.\n",
      "\n",
      "Skipping denoiser training for this quick test\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    USE_SMALL_DATASET = True\n",
    "    \n",
    "    if USE_SMALL_DATASET:\n",
    "        print(\"Testing with a small dataset...\")\n",
    "        test_captions = load_captions(CAPTION_FILE)\n",
    "        sample_keys = list(test_captions.keys())[:100]  \n",
    "        small_captions = {k: test_captions[k] for k in sample_keys}\n",
    "        \n",
    "        dataset = FlickrDataset(IMAGE_DIR, small_captions, transform=transform)\n",
    "        \n",
    "        for key in sample_keys:\n",
    "            feature_path = os.path.join(FEATURE_DIR, os.path.splitext(key)[0] + '.npy')\n",
    "            if not os.path.exists(feature_path):\n",
    "                extract_features(dataset, FEATURE_DIR, batch_size=8)\n",
    "                break\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        initialize_tokenizer()\n",
    "        \n",
    "        # Create dataset with features and captions\n",
    "        feature_dataset = FeatureCaptionDataset(FEATURE_DIR, small_captions, tokenizer)\n",
    "        \n",
    "        # Split data\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            range(len(feature_dataset)), test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create DataLoader\n",
    "        train_loader = DataLoader(\n",
    "            [feature_dataset[i] for i in train_indices],\n",
    "            batch_size=8,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            [feature_dataset[i] for i in val_indices],\n",
    "            batch_size=4\n",
    "        )\n",
    "        \n",
    "        caption_model = GPT2CaptionModel().to(device)\n",
    "        \n",
    "        # Training will take time even with small dataset\n",
    "        print(\"Training caption model (this may take some time even with small dataset)...\")\n",
    "        caption_model = train_caption_model(\n",
    "            caption_model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=5,  \n",
    "            lr=5e-5\n",
    "        )\n",
    "        \n",
    "        # Evaluate caption model\n",
    "        print(\"Evaluating caption model...\")\n",
    "        caption_samples, caption_metrics = evaluate_caption_model(\n",
    "            caption_model,\n",
    "            val_loader,\n",
    "            num_samples=5\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        if caption_samples:\n",
    "            print(\"\\nCaption Results:\")\n",
    "            for i, sample in enumerate(caption_samples):\n",
    "                print(f\"Example {i+1}:\")\n",
    "                print(f\"True: {sample['true']}\")\n",
    "                print(f\"Generated: {sample['generated']}\")\n",
    "                print()\n",
    "        \n",
    "        # Skip denoiser training for this quick test\n",
    "        print(\"Skipping denoiser training for this quick test\")\n",
    "    else:\n",
    "        results = main()\n",
    "        if 'caption_model' in results and 'denoiser_model' in results:\n",
    "            interface = setup_gradio_interface(results['caption_model'], results['denoiser_model'])\n",
    "            interface.launch(share=True)\n",
    "        else:\n",
    "            print(\"Model training failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b804f5a3-27c0-4872-bf51-1e5792d7baed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with a small dataset...\n",
      "Loaded captions for 8091 images\n",
      "Initializing GPT-2 tokenizer...\n",
      "Tokenizer vocabulary size: 50257\n",
      "Training caption model (this may take some time even with small dataset)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  10%|███▎                             | 1/10 [00:00<00:07,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 7.3777\n",
      "Sample caption: The following was written by Richard A.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 2.2931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8904\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "New best model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_best.pt\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.9253\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average Loss: 0.8196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8100\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "New best model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_best.pt\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.7606\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average Loss: 0.6370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7837\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "New best model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_best.pt\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.6139\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average Loss: 0.4797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8464\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.5601\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average Loss: 0.3309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9566\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_5.pt\n",
      "Final model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_final.pt\n",
      "Loaded best model based on validation loss\n",
      "Evaluating caption model...\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "\n",
      "Caption Generation Examples:\n",
      "Example 1:\n",
      "True: two dogs running around\n",
      "Generated: A photo.\n",
      "\n",
      "Example 2:\n",
      "True: A dog and a tennis ball .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 3:\n",
      "True: Two people wearing yellow jackets cross-country skiing .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 4:\n",
      "True: A little kid is jumping off a high dive at the pool .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 5:\n",
      "True: Two dogs playing on a beach .\n",
      "Generated: A photo.\n",
      "\n",
      "Metrics:\n",
      "BLEU-1: 0.0086\n",
      "BLEU-4: 0.0026\n",
      "\n",
      "Caption Results:\n",
      "Example 1:\n",
      "True: two dogs running around\n",
      "Generated: A photo.\n",
      "\n",
      "Example 2:\n",
      "True: A dog and a tennis ball .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 3:\n",
      "True: Two people wearing yellow jackets cross-country skiing .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 4:\n",
      "True: A little kid is jumping off a high dive at the pool .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 5:\n",
      "True: Two dogs playing on a beach .\n",
      "Generated: A photo.\n",
      "\n",
      "Skipping denoiser training for this quick test\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # For testing with a small dataset\n",
    "    USE_SMALL_DATASET = True\n",
    "    \n",
    "    if USE_SMALL_DATASET:\n",
    "        print(\"Testing with a small dataset...\")\n",
    "        test_captions = load_captions(CAPTION_FILE)\n",
    "        sample_keys = list(test_captions.keys())[:100]  # Use only 100 images\n",
    "        small_captions = {k: test_captions[k] for k in sample_keys}\n",
    "        \n",
    "        # Create dataset for feature extraction\n",
    "        dataset = FlickrDataset(IMAGE_DIR, small_captions, transform=transform)\n",
    "        \n",
    "        # Extract features if needed\n",
    "        for key in sample_keys:\n",
    "            feature_path = os.path.join(FEATURE_DIR, os.path.splitext(key)[0] + '.npy')\n",
    "            if not os.path.exists(feature_path):\n",
    "                extract_features(dataset, FEATURE_DIR, batch_size=8)\n",
    "                break\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        initialize_tokenizer()\n",
    "        \n",
    "        # Create dataset with features and captions\n",
    "        feature_dataset = FeatureCaptionDataset(FEATURE_DIR, small_captions, tokenizer)\n",
    "        \n",
    "        # Split data\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            range(len(feature_dataset)), test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create DataLoader\n",
    "        train_loader = DataLoader(\n",
    "            [feature_dataset[i] for i in train_indices],\n",
    "            batch_size=8,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            [feature_dataset[i] for i in val_indices],\n",
    "            batch_size=4\n",
    "        )\n",
    "        \n",
    "        # Train caption model (with limited epochs)\n",
    "        caption_model = GPT2CaptionModel().to(device)\n",
    "        \n",
    "\n",
    "        print(\"Training caption model (this may take some time even with small dataset)...\")\n",
    "        caption_model = train_caption_model(\n",
    "            caption_model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=5,  \n",
    "            lr=5e-5\n",
    "        )\n",
    "        \n",
    "        # Evaluate caption model\n",
    "        print(\"Evaluating caption model...\")\n",
    "        caption_samples, caption_metrics = evaluate_caption_model(\n",
    "            caption_model,\n",
    "            val_loader,\n",
    "            num_samples=5\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        if caption_samples:\n",
    "            print(\"\\nCaption Results:\")\n",
    "            for i, sample in enumerate(caption_samples):\n",
    "                print(f\"Example {i+1}:\")\n",
    "                print(f\"True: {sample['true']}\")\n",
    "                print(f\"Generated: {sample['generated']}\")\n",
    "                print()\n",
    "        \n",
    "        print(\"Skipping denoiser training for this quick test\")\n",
    "    else:\n",
    "        # Run full training\n",
    "        results = main()\n",
    "        \n",
    "        # Check if models were created successfully\n",
    "        if 'caption_model' in results and 'denoiser_model' in results:\n",
    "            # Set up and launch Gradio interface\n",
    "            interface = setup_gradio_interface(results['caption_model'], results['denoiser_model'])\n",
    "            interface.launch(share=True)\n",
    "        else:\n",
    "            print(\"Model training failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7aaf90e-de94-4bb8-92db-442954ffd511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with a small dataset...\n",
      "Loaded captions for 8091 images\n",
      "Initializing GPT-2 tokenizer...\n",
      "Tokenizer vocabulary size: 50257\n",
      "Training caption model (this may take some time even with small dataset)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|                                         | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 7.4206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  10%|███▎                             | 1/10 [00:01<00:11,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample caption: \n",
      "A couple weeks ago I wrote a blog post about the very high energy that comes with having a strong personal connection with an organisation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|████████████████████████████████| 10/10 [00:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 2.1546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9640\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "New best model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_best.pt\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 1.0241\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average Loss: 0.7573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8717\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "New best model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_best.pt\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.8254\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average Loss: 0.5858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8715\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "New best model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_best.pt\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.6071\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average Loss: 0.4349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9420\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  10%|███▎                             | 1/10 [00:00<00:05,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10, Loss: 0.5112\n",
      "Sample caption: A photo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|████████████████████████████████| 10/10 [00:05<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average Loss: 0.2921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████| 5/5 [00:00<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1021\n",
      "Sample validation captions:\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Generated: A photo.\n",
      "Checkpoint saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_epoch_5.pt\n",
      "Final model saved to /Users/nehaeshwaragari/Documents/Deep Learning/Project 3/Flickr8K/output/checkpoints/caption_model_final.pt\n",
      "Loaded best model based on validation loss\n",
      "Evaluating caption model...\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "Feature shape: torch.Size([1, 2048])\n",
      "Generated caption: 'A photo.'\n",
      "\n",
      "Caption Generation Examples:\n",
      "Example 1:\n",
      "True: A blond dog and a black and white dog run in a dirt field .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 2:\n",
      "True: A black and white dog catches a toy in midair .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 3:\n",
      "True: Two skiers are sliding down a trail in the woods .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 4:\n",
      "True: A little kid is jumping off a high dive at the pool .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 5:\n",
      "True: Two dogs run towards each other on a rocky area with water in the background .\n",
      "Generated: A photo.\n",
      "\n",
      "Metrics:\n",
      "BLEU-1: 0.0017\n",
      "BLEU-4: 0.0005\n",
      "\n",
      "Caption Results:\n",
      "Example 1:\n",
      "True: A blond dog and a black and white dog run in a dirt field .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 2:\n",
      "True: A black and white dog catches a toy in midair .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 3:\n",
      "True: Two skiers are sliding down a trail in the woods .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 4:\n",
      "True: A little kid is jumping off a high dive at the pool .\n",
      "Generated: A photo.\n",
      "\n",
      "Example 5:\n",
      "True: Two dogs run towards each other on a rocky area with water in the background .\n",
      "Generated: A photo.\n",
      "\n",
      "Skipping denoiser training for this quick test\n"
     ]
    }
   ],
   "source": [
    "def setup_gradio_interface(caption_model, denoiser_model):\n",
    "    def process_image(input_image):\n",
    "        input_image_pil = Image.fromarray(input_image).convert('RGB')\n",
    "        \n",
    "        input_tensor = transform(input_image_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "            feature_extractor = nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
    "            feature_extractor.eval()\n",
    "            features = feature_extractor(input_tensor).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        # Generate caption\n",
    "        caption = caption_model.generate_caption(features, temperature=0.7)\n",
    "        \n",
    "        # Process for denoising\n",
    "        noisy_tensor = diffusion_transform(input_image_pil).unsqueeze(0).to(device)\n",
    "        noisy_tensor = add_noise_to_image(noisy_tensor, noise_level=0.2)\n",
    "        \n",
    "        # Denoise\n",
    "        with torch.no_grad():\n",
    "            denoised_tensor = denoiser_model(noisy_tensor)\n",
    "        \n",
    "        # Convert tensors to images\n",
    "        noisy_image = tensor_to_pil(noisy_tensor)\n",
    "        denoised_image = tensor_to_pil(denoised_tensor)\n",
    "        \n",
    "        return caption, noisy_image, denoised_image\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    interface = gr.Interface(\n",
    "        fn=process_image,\n",
    "        inputs=gr.Image(),\n",
    "        outputs=[\n",
    "            gr.Textbox(label=\"Generated Caption\"),\n",
    "            gr.Image(label=\"Noisy Image\"),\n",
    "            gr.Image(label=\"Denoised Image\")\n",
    "        ],\n",
    "        title=\"Image Captioning and Denoising Demo\",\n",
    "        description=\"Upload an image to generate a caption and see denoising in action.\"\n",
    "    )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    USE_SMALL_DATASET = True\n",
    "    \n",
    "    if USE_SMALL_DATASET:\n",
    "        print(\"Testing with a small dataset...\")\n",
    "        test_captions = load_captions(CAPTION_FILE)\n",
    "        sample_keys = list(test_captions.keys())[:100] \n",
    "        small_captions = {k: test_captions[k] for k in sample_keys}\n",
    "        \n",
    "        # Create dataset for feature extraction\n",
    "        dataset = FlickrDataset(IMAGE_DIR, small_captions, transform=transform)\n",
    "        \n",
    "        # Extract features if needed\n",
    "        for key in sample_keys:\n",
    "            feature_path = os.path.join(FEATURE_DIR, os.path.splitext(key)[0] + '.npy')\n",
    "            if not os.path.exists(feature_path):\n",
    "                extract_features(dataset, FEATURE_DIR, batch_size=8)\n",
    "                break\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        initialize_tokenizer()\n",
    "        \n",
    "        # Create dataset with features and captions\n",
    "        feature_dataset = FeatureCaptionDataset(FEATURE_DIR, small_captions, tokenizer)\n",
    "        \n",
    "        # Split data\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            range(len(feature_dataset)), test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create DataLoader\n",
    "        train_loader = DataLoader(\n",
    "            [feature_dataset[i] for i in train_indices],\n",
    "            batch_size=8,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            [feature_dataset[i] for i in val_indices],\n",
    "            batch_size=4\n",
    "        )\n",
    "        \n",
    "        # Train caption model (with limited epochs)\n",
    "        caption_model = GPT2CaptionModel().to(device)\n",
    "        \n",
    "        # Training will take time even with small dataset\n",
    "        print(\"Training caption model (this may take some time even with small dataset)...\")\n",
    "        caption_model = train_caption_model(\n",
    "            caption_model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=5,  \n",
    "            lr=5e-5\n",
    "        )\n",
    "        \n",
    "        # Evaluate caption model\n",
    "        print(\"Evaluating caption model...\")\n",
    "        caption_samples, caption_metrics = evaluate_caption_model(\n",
    "            caption_model,\n",
    "            val_loader,\n",
    "            num_samples=5\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        if caption_samples:\n",
    "            print(\"\\nCaption Results:\")\n",
    "            for i, sample in enumerate(caption_samples):\n",
    "                print(f\"Example {i+1}:\")\n",
    "                print(f\"True: {sample['true']}\")\n",
    "                print(f\"Generated: {sample['generated']}\")\n",
    "                print()\n",
    "        \n",
    "        # Skip denoiser training for this quick test\n",
    "        print(\"Skipping denoiser training for this quick test\")\n",
    "    else:\n",
    "        # Run full training\n",
    "        results = main()\n",
    "        \n",
    "        # Check if models were created successfully\n",
    "        if 'caption_model' in results and 'denoiser_model' in results:\n",
    "            # Set up and launch Gradio interface\n",
    "            interface = setup_gradio_interface(results['caption_model'], results['denoiser_model'])\n",
    "            interface.launch(share=True)\n",
    "        else:\n",
    "            print(\"Model training failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52158c2a-9f7c-4a05-b303-8ac56be08863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gradio_interface(caption_model, denoiser_model):\n",
    "    def process_image(input_image):\n",
    "        input_image_pil = Image.fromarray(input_image).convert('RGB')\n",
    "        \n",
    "        input_tensor = transform(input_image_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "            feature_extractor = nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
    "            feature_extractor.eval()\n",
    "            features = feature_extractor(input_tensor).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        # Generate caption\n",
    "        caption = caption_model.generate_caption(features, temperature=0.7)\n",
    "        \n",
    "        # Process for denoising\n",
    "        noisy_tensor = diffusion_transform(input_image_pil).unsqueeze(0).to(device)\n",
    "        noisy_tensor = add_noise_to_image(noisy_tensor, noise_level=0.2)\n",
    "        \n",
    "        # Denoise\n",
    "        with torch.no_grad():\n",
    "            denoised_tensor = denoiser_model(noisy_tensor)\n",
    "        \n",
    "        # Convert tensors to images\n",
    "        noisy_image = tensor_to_pil(noisy_tensor)\n",
    "        denoised_image = tensor_to_pil(denoised_tensor)\n",
    "        \n",
    "        return caption, noisy_image, denoised_image\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    interface = gr.Interface(\n",
    "        fn=process_image,\n",
    "        inputs=gr.Image(),\n",
    "        outputs=[\n",
    "            gr.Textbox(label=\"Generated Caption\"),\n",
    "            gr.Image(label=\"Noisy Image\"),\n",
    "            gr.Image(label=\"Denoised Image\")\n",
    "        ],\n",
    "        title=\"Image Captioning and Denoising Demo\",\n",
    "        description=\"Upload an image to generate a caption and see denoising in action.\"\n",
    "    )\n",
    "    \n",
    "    return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5bd6650e-5764-431e-be79-66db38c41922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 15:52:38,649 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-20 15:52:38,658 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 15:52:39,217 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-20 15:52:39,353 - INFO - HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://e94784f02a6e0e894b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 15:52:40,532 - INFO - HTTP Request: HEAD https://e94784f02a6e0e894b.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e94784f02a6e0e894b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    caption_model = GPT2CaptionModel().to(device)\n",
    "    denoiser_model = SimpleDenoiser().to(device)\n",
    "    \n",
    "    # Launch the interface\n",
    "    interface = setup_gradio_interface(caption_model, denoiser_model)\n",
    "    interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
